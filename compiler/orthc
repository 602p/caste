#!/usr/bin/python3

import sys, drawblockdiag, tokenize, parse, grammarutil, json, os, traceback

#This is the actual compiler. It just chains together the tokenizing step,
#the parsing step, and the transforming step to produce an IR file. Then it
#invokes LLC and GCC to create a actual exectuable

_print=print
def print(*a, **k): #Hack so that messages don't get truncated by a crash
	_print(*a, **k)
	sys.stdout.flush()

print("Reading...                       ", end="")
string=open(sys.argv[1], 'r').read()

print("\rTokenizing...                       ", end="")
tokens=tokenize.tokenize(string, sys.argv[1])

print("\rTokenizing... [Dumping Tokens]             ", end="")
with open('tokens.txt', 'w') as fd:
	fd.write("\n".join(str(t) for t in tokens))

print("\rParsing...                       ", end="")
node=parse.parse(tokens)

print("\rParsing... [Drawing BD]                 ", end="")
drawblockdiag.emit_node_file(open('out.bd', 'w'), node, portrait_mode=True)

print("\rPreparing Transformer...                       ", end="")
import transform, transformer, datamodel

print("\rTransforming...                       \n", end="")
with open("out.ll", 'w') as fd:
	e=transform.Emitter(fd,{
		"no0double":"no0double" in sys.argv,
		"path":sys.path+["."]
	})
	with e.scope():
		with e.context(file="in"):
			try:
				transform.emit_project(e, node)
			except BaseException as ex:
				print()
				print("*****************************************\n"*2, end='')
				print("******************ERROR******************")
				print("*****************************************\n"*2)
				print(ex, type(ex))
				print("SCOPES: (most recent first)")
				for map in e.scopes.maps:
					print("\t"+str(map))
				print("CONTEX: (most recent first)")
				for map in e.context_map.maps:
					print("\t"+str(map))
				print("SIGNAT: ", e.signatures)
				print("INCLUD: ", e.included_files)
				print("TYPES : ", end="")
				for type in e.types.values():
					print(str(type)+" -> "+type.get_llvm_representation(), end=";  ")
				print("\nLASTLI: ", e.last_line)
				print("LASTFI: ", e.last_file)
				print("LASTME: ", e.last_method)
				raise ex #Workaround. Normally one would use a traceback.* method here to pretty-print
							#the error and continue, but for some reason the stdlib requires a module
							#`tokenize` for that and since i have one in my working directory/sys.modules
							#cache it gets that instead (which obviously dosen't expose the same functionaliy
							#and therefore is bust)

	e.emit_global_statments()

if not "nobuild" in sys.argv:
	print("\rCompiling...                       ", end="")
	assert os.system("llc out.ll;")==0, "Command Failed"

if not "nolink" in sys.argv:
	print("\rLinking...                       ", end="")
	assert os.system("gcc -o "+(sys.argv[2] if len(sys.argv)==3 else "out")+" out.s; rm out.s")==0, "Command Failed"

print("\r\033[92mDone!\033[0m                                        ")